@*<h2>Speech to Text (Microphone)</h2>

    <button id="startBtn">Start Recording</button>
    <button id="stopBtn" disabled>Stop Recording</button>

    <p id="status"></p>
    <div id="result" style="margin-top: 20px; font-weight: bold;"></div>*@

@{ ViewData["Title"] = "Speech to Text Recorder"; }

<style>
    html {
        font-size: 14px;
        position: relative;
        min-height: 100%;
    }
    
     .speech-recorder {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
    }

    .recorder-visualizer {
        width: 100%;
        height: 60px;
        background-color: #f8f9fa;
        border-radius: 4px;
        margin-bottom: 1rem;
    }

    .recorder-controls {
        display: flex;
        justify-content: center;
        gap: 10px;
        margin-bottom: 20px;
    }

    .mic-button {
        width: 64px;
        height: 64px;
        border-radius: 50%;
        border: none;
        background-color: #007bff;
        color: white;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.3s ease;
    }

    .mic-button.recording {
        background-color: #dc3545;
        animation: pulse 1.5s infinite;
    }

    .answer-container {
        margin-top: 20px;
        padding: 15px;
        background-color: #e9ecef;
        border-radius: 4px;
        display: none;
    }

    #result {
        min-height: 100px;
        white-space: pre-wrap;
        font-size: 1.1rem;
    }

    @@keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.1); }
        100% { transform: scale(1); }
    }
</style>

<div class="speech-recorder">
    <h2>Speech to Text Recorder</h2>

    <div class="recorder-status mt-4">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Recorder Status</h5>
                <p id="status" class="card-text">Click the microphone to start speaking</p>

                <div class="d-flex justify-content-center mb-3">
                    <div id="visualizer" class="recorder-visualizer"></div>
                </div>

                <div class="recorder-controls">
                    <button id="micButton" class="mic-button">
                        <i class="fas fa-microphone fa-2x"></i>
                    </button>
                    <button id="stop-micButton" class="mic-button">
                        <i class="fa fa-microphone-slash fa-2x"></i>
                    </button>
                </div>
            </div>
        </div>
    </div>

    <div class="transcription-result mt-4">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Transcription Result</h5>
                <div id="result-container">
                    <p id="result-placeholder" class="text-muted">Your transcription will appear here after recording</p>
                    <div id="result" class="p-3 bg-light rounded d-none"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="answer-container mt-4" id="answer-container">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Answer</h5>
                <p id="answer-text"></p>
                <audio id="answer-audio" controls class="w-100"></audio>
            </div>
        </div>
    </div>
</div>

<script src="https://kit.fontawesome.com/4ad1b9abc0.js" crossorigin="anonymous"></script>

    <script>
        

        let mediaRecorder;
        let audioChunks = [];
        let audioStream;
        let audioContext;
        let analyser;
        let visualizerCanvas;
        let canvasContext;
        let animationId;
        let silenceTimeout;
        let isRecording = false;

        const micButton = document.getElementById('micButton');
        const statusEl = document.getElementById('status');
        const resultEl = document.getElementById('result');
        const resultPlaceholder = document.getElementById('result-placeholder');
        const visualizer = document.getElementById('visualizer');
        const stopRec = document.getElementById('stop-micButton');

        function setupVisualizer() {
            visualizerCanvas = document.createElement('canvas');
            visualizerCanvas.width = visualizer.clientWidth;
            visualizerCanvas.height = 60;
            visualizer.appendChild(visualizerCanvas);
            canvasContext = visualizerCanvas.getContext('2d');
        }

        function init() {
            setupVisualizer();
            micButton.addEventListener('click', toggleRecording);
            

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                statusEl.textContent = 'Your browser does not support audio recording.';
                micButton.disabled = true;
            }
        }


        function drawVisualizer() {
            if (!analyser) return;

            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            analyser.getByteTimeDomainData(dataArray);

            canvasContext.clearRect(0, 0, visualizerCanvas.width, visualizerCanvas.height);
            canvasContext.lineWidth = 2;
            canvasContext.strokeStyle = '#007bff';
            canvasContext.beginPath();

            const sliceWidth = visualizerCanvas.width / bufferLength;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                const v = dataArray[i] / 128.0;
                const y = v * visualizerCanvas.height / 2;

                if (i === 0) {
                    canvasContext.moveTo(x, y);
                } else {
                    canvasContext.lineTo(x, y);
                }

                x += sliceWidth;
            }

            canvasContext.lineTo(visualizerCanvas.width, visualizerCanvas.height / 2);
            canvasContext.stroke();
            animationId = requestAnimationFrame(drawVisualizer);
        }

        async function toggleRecording() {
            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }

        async function startRecording() {
            try {
                audioChunks = [];
                resultEl.textContent = '';
                resultEl.classList.add('d-none');
                resultPlaceholder.classList.remove('d-none');
                document.getElementById('answer-container').style.display = 'none';

                statusEl.textContent = 'Requesting microphone access...';
                micButton.disabled = true;

                audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(audioStream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048; //2048
                source.connect(analyser);

                drawVisualizer();

                const mimeType = MediaRecorder.isTypeSupported('audio/webm')
                    ? 'audio/webm'
                    : 'audio/mp4';

                mediaRecorder = new MediaRecorder(audioStream, { mimeType });

                mediaRecorder.addEventListener('dataavailable', event => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                });

                mediaRecorder.addEventListener('start', () => {
                    //isRecording = true;
                    statusEl.textContent = 'Recording in progress... Speak now';
                    micButton.classList.add('recording');
                });

                isRecording = true;

                mediaRecorder.addEventListener('stop', sendAudioToServer);

                mediaRecorder.start(1000);
                startSilenceDetection();
            } catch (error) {
                console.error('Error starting recording:', error);
                statusEl.textContent = `Error: ${error.message}`;
                micButton.disabled = false;
            }
        }

        function startSilenceDetection() {
            const bufferLength = analyser.fftSize;
            const dataArray = new Uint8Array(bufferLength);
            let silenceStart = null;
            const silenceThreshold = 5; 

            function checkSilence() {
                if (!isRecording) return;

                analyser.getByteTimeDomainData(dataArray);
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    let val = (dataArray[i] - 128) / 128;
                    sum += val * val;
                }
                const rms = Math.sqrt(sum / bufferLength) * 100;

                // Debug output
                console.log(`RMS: ${rms.toFixed(2)}, silenceStart: ${silenceStart}`);

                if (rms < silenceThreshold) {
                    if (!silenceStart) {
                        silenceStart = Date.now();
                    } else if (Date.now() - silenceStart > 3000) {
                        console.log("Silence detected, stopping recording.");
                        stopRecording();
                        return;
                    }
                } else {
                    silenceStart = null;
                }

                requestAnimationFrame(checkSilence);
            }

            checkSilence();
        }

        //console.log(`RMS: ${ rms.toFixed(2) }`);

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                statusEl.textContent = 'Processing recording...';
                mediaRecorder.stop();
                micButton.classList.remove('recording');
                isRecording = false;

                if (audioStream) {
                    audioStream.getTracks().forEach(track => track.stop());
                }

                if (animationId) {
                    cancelAnimationFrame(animationId);
                }
            }
        }

        async function convertBlobToWav(blob) {
            const arrayBuffer = await blob.arrayBuffer();
            const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);

            const numOfChan = audioBuffer.numberOfChannels;
            const length = audioBuffer.length * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            const sampleRate = audioBuffer.sampleRate;
            let offset = 0;

            function setUint16(data) {
                view.setUint16(offset, data, true);
                offset += 2;
            }

            function setUint32(data) {
                view.setUint32(offset, data, true);
                offset += 4;
            }

            setUint32(0x46464952);
            setUint32(length - 8);
            setUint32(0x45564157);
            setUint32(0x20746d66);
            setUint32(16);
            setUint16(1);
            setUint16(numOfChan);
            setUint32(sampleRate);
            setUint32(sampleRate * 2 * numOfChan);
            setUint16(numOfChan * 2);
            setUint16(16);
            setUint32(0x61746164);
            setUint32(length - offset - 4);

            for (let i = 0; i < numOfChan; i++) {
                channels.push(audioBuffer.getChannelData(i));
            }

            for (let i = 0; i < audioBuffer.length; i++) {
                for (let ch = 0; ch < numOfChan; ch++) {
                    let sample = Math.max(-1, Math.min(1, channels[ch][i]));
                    sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                    view.setInt16(offset, sample, true);
                    offset += 2;
                }
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        async function sendAudioToServer() {
            try {
                statusEl.textContent = 'Processing audio...';

                if (audioChunks.length === 0) {
                    throw new Error("No audio data was captured during recording");
                }

                const rawBlob = new Blob(audioChunks, { type: mediaRecorder.mimeType });
                const audioBlob = await convertBlobToWav(rawBlob);
                const filename = `recording_${Date.now()}.wav`;

                var formData = new FormData();
                formData.append('audioFile', audioBlob, filename);

                statusEl.textContent = 'Sending to server for transcription...';

                const response = await fetch('/Voice/STTs', {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`Server returned ${response.status}`);
                }

                const data = await response.json();

                resultPlaceholder.classList.add('d-none');
                resultEl.classList.remove('d-none');
                resultEl.textContent = data.transcription || 'No transcription returned';

                const answerContainer = document.getElementById('answer-container');
                const answerText = document.getElementById('answer-text');
                const answerAudio = document.getElementById('answer-audio');

                answerText.textContent = data.answer;
                answerAudio.src = data.audioUrl;
                answerContainer.style.display = 'block';

                answerAudio.play();

                statusEl.textContent = 'Ready to record again';
                micButton.disabled = false;

            } catch (error) {
                console.error('Error processing audio:', error);
                statusEl.textContent = `Error: ${error.message}`;
                micButton.disabled = false;

                resultPlaceholder.classList.add('d-none');
                resultEl.classList.remove('d-none');
                resultEl.innerHTML = `<div class="text-danger">Error: ${error.message}</div>`;
            }
        }

        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', init);
        } else {
            init();
        }


    </script>




