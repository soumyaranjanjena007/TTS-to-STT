@*<h2>Speech to Text (Microphone)</h2>

    <button id="startBtn">Start Recording</button>
    <button id="stopBtn" disabled>Stop Recording</button>

    <p id="status"></p>
    <div id="result" style="margin-top: 20px; font-weight: bold;"></div>*@

@{ ViewData["Title"] = "Speech to Text Recorder"; }

<style>
    html {
        font-size: 14px;
        position: relative;
        min-height: 100%;
    }
    /* Base styles */
    /*

    @@media (min-width: 768px) {
        html {
            font-size: 16px;
        }
    }

    body {
        margin-bottom: 60px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }*/

    /* Footer */
    /*.footer {
        position: absolute;
        bottom: 0;
        width: 100%;
        white-space: nowrap;
        line-height: 60px;
    }*/

    /* Speech recorder styles */
    /*.speech-recorder {
        max-width: 800px;
        margin: 0 auto;
    }

    .recorder-visualizer {
        width: 100%;
        height: 60px;
        background-color: #f8f9fa;
        border-radius: 4px;
        margin-bottom: 1rem;
    }

    .recorder-controls {
        display: flex;
        justify-content: center;
        gap: 10px;
    }

    #result {
        min-height: 100px;
        white-space: pre-wrap;
        font-size: 1.1rem;
    }*/

    /* Status indicators */
    /*.recording-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        background-color: #dc3545;
        margin-right: 5px;
        animation: pulse 1.5s infinite;
    }

    @@keyframes pulse {
        0% {
            opacity: 1;
        }

        50% {
            opacity: 0.5;
        }

        100% {
            opacity: 1;
        }
    }*/


     .speech-recorder {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
    }

    .recorder-visualizer {
        width: 100%;
        height: 60px;
        background-color: #f8f9fa;
        border-radius: 4px;
        margin-bottom: 1rem;
    }

    .recorder-controls {
        display: flex;
        justify-content: center;
        gap: 10px;
        margin-bottom: 20px;
    }

    .mic-button {
        width: 64px;
        height: 64px;
        border-radius: 50%;
        border: none;
        background-color: #007bff;
        color: white;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.3s ease;
    }

    .mic-button.recording {
        background-color: #dc3545;
        animation: pulse 1.5s infinite;
    }

    .answer-container {
        margin-top: 20px;
        padding: 15px;
        background-color: #e9ecef;
        border-radius: 4px;
        display: none;
    }

    #result {
        min-height: 100px;
        white-space: pre-wrap;
        font-size: 1.1rem;
    }

    @@keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.1); }
        100% { transform: scale(1); }
    }
</style>

@*<div class="speech-recorder">
    <h2>Speech to Text Recorder</h2>

    <div class="recorder-status mt-4">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Recorder Status</h5>
                <p id="status" class="card-text">Ready to record</p>

                <div class="d-flex justify-content-center mb-3">
                    <div id="visualizer" class="recorder-visualizer"></div>
                </div>

                <div class="recorder-controls">
                    <button id="startBtn" class="btn btn-primary me-2">
                        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-mic-fill" viewBox="0 0 16 16">
                            <path d="M5 3a3 3 0 0 1 6 0v5a3 3 0 0 1-6 0V3z" />
                            <path d="M3.5 6.5A.5.5 0 0 1 4 7v1a4 4 0 0 0 8 0V7a.5.5 0 0 1 1 0v1a5 5 0 0 1-4.5 4.975V15h3a.5.5 0 0 1 0 1h-7a.5.5 0 0 1 0-1h3v-2.025A5 5 0 0 1 3 8V7a.5.5 0 0 1 .5-.5z" />
                        </svg>
                        Start Recording
                    </button>
                    <button id="stopBtn" class="btn btn-danger" disabled>
                        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-stop-fill" viewBox="0 0 16 16">
                            <path d="M5 3.5h6A1.5 1.5 0 0 1 12.5 5v6a1.5 1.5 0 0 1-1.5 1.5H5A1.5 1.5 0 0 1 3.5 11V5A1.5 1.5 0 0 1 5 3.5z" />
                        </svg>
                        Stop Recording
                    </button>
                </div>

                <div id="audio-playback" class="mt-3 d-none">
                    <h6>Preview Recording</h6>
                    <audio id="audio-preview" controls class="w-100"></audio>
                </div>
            </div>
        </div>
    </div>

    <div class="transcription-result mt-4">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Transcription Result</h5>
                <div id="result-container">
                    <p id="result-placeholder" class="text-muted">Your transcription will appear here after recording</p>
                    <div id="result" class="p-3 bg-light rounded d-none"></div>
                </div>
            </div>
        </div>
    </div>
</div>*@

<div class="speech-recorder">
    <h2>Speech to Text Recorder</h2>

    <div class="recorder-status mt-4">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Recorder Status</h5>
                <p id="status" class="card-text">Click the microphone to start speaking</p>

                <div class="d-flex justify-content-center mb-3">
                    <div id="visualizer" class="recorder-visualizer"></div>
                </div>

                <div class="recorder-controls">
                    <button id="micButton" class="mic-button">
                        <i class="fas fa-microphone fa-2x"></i>
                    </button>
                    <button id="stop-micButton" class="mic-button">
                        <i class="fa fa-microphone-slash fa-2x"></i>
                    </button>
                </div>
            </div>
        </div>
    </div>

    <div class="transcription-result mt-4">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Transcription Result</h5>
                <div id="result-container">
                    <p id="result-placeholder" class="text-muted">Your transcription will appear here after recording</p>
                    <div id="result" class="p-3 bg-light rounded d-none"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="answer-container mt-4" id="answer-container">
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Answer</h5>
                <p id="answer-text"></p>
                <audio id="answer-audio" controls class="w-100"></audio>
            </div>
        </div>
    </div>
</div>

<script src="https://kit.fontawesome.com/4ad1b9abc0.js" crossorigin="anonymous"></script>

    <script>
        //(function () {
        //    // Global variables
        //    let mediaRecorder;
        //    let audioChunks = [];
        //    let audioStream;
        //    let audioContext;
        //    let analyser;
        //    let visualizerCanvas;
        //    let canvasContext;
        //    let animationId;

        //    // DOM elements
        //    const startBtn = document.getElementById('startBtn');
        //    const stopBtn = document.getElementById('stopBtn');
        //    const statusEl = document.getElementById('status');
        //    const resultEl = document.getElementById('result');
        //    const resultPlaceholder = document.getElementById('result-placeholder');
        //    const audioPlayback = document.getElementById('audio-playback');
        //    const audioPreview = document.getElementById('audio-preview');
        //    const visualizer = document.getElementById('visualizer');

        //    // Set up visualizer canvas
        //    function setupVisualizer() {
        //        visualizerCanvas = document.createElement('canvas');
        //        visualizerCanvas.width = visualizer.clientWidth;
        //        visualizerCanvas.height = 60;
        //        visualizer.appendChild(visualizerCanvas);
        //        canvasContext = visualizerCanvas.getContext('2d');
        //    }

        //    // Initialize event listeners
        //    function init() {
        //        setupVisualizer();

        //        startBtn.addEventListener('click', startRecording);
        //        stopBtn.addEventListener('click', stopRecording);

        //        // Check if browser supports audio recording
        //        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        //            statusEl.textContent = 'Your browser does not support audio recording.';
        //            startBtn.disabled = true;
        //            return;
        //        }
        //    }

        //    // Draw the audio visualizer
        //    function drawVisualizer() {
        //        if (!analyser) return;

        //        const bufferLength = analyser.frequencyBinCount;
        //        const dataArray = new Uint8Array(bufferLength);
        //        analyser.getByteTimeDomainData(dataArray);

        //        canvasContext.clearRect(0, 0, visualizerCanvas.width, visualizerCanvas.height);
        //        canvasContext.lineWidth = 2;
        //        canvasContext.strokeStyle = '#007bff';
        //        canvasContext.beginPath();

        //        const sliceWidth = visualizerCanvas.width / bufferLength;
        //        let x = 0;

        //        for (let i = 0; i < bufferLength; i++) {
        //            const v = dataArray[i] / 128.0;
        //            const y = v * visualizerCanvas.height / 2;

        //            if (i === 0) {
        //                canvasContext.moveTo(x, y);
        //            } else {
        //                canvasContext.lineTo(x, y);
        //            }

        //            x += sliceWidth;
        //        }

        //        canvasContext.lineTo(visualizerCanvas.width, visualizerCanvas.height / 2);
        //        canvasContext.stroke();
        //        animationId = requestAnimationFrame(drawVisualizer);
        //    }

        //    // Start audio recording
        //    async function startRecording() {
        //        try {
        //            // Reset previous recording
        //            audioChunks = [];
        //            resultEl.textContent = '';
        //            resultEl.classList.add('d-none');
        //            resultPlaceholder.classList.remove('d-none');
        //            audioPlayback.classList.add('d-none');

        //            // Update UI
        //            statusEl.textContent = 'Requesting microphone access...';
        //            startBtn.disabled = true;

        //            // Get audio stream
        //            audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });

        //            // Set up audio context and analyser for visualization
        //            audioContext = new (window.AudioContext || window.webkitAudioContext)();
        //            const source = audioContext.createMediaStreamSource(audioStream);
        //            analyser = audioContext.createAnalyser();
        //            analyser.fftSize = 2048;
        //            source.connect(analyser);

        //            // Start visualization
        //            drawVisualizer();

        //            // Create and configure MediaRecorder
        //            // Check for supported MIME types
        //            const mimeTypes = [
        //                'audio/webm;codecs=opus',
        //                'audio/webm',
        //                'audio/ogg;codecs=opus',
        //                'audio/mp4'
        //            ];

        //            let mimeType = '';
        //            for (let type of mimeTypes) {
        //                if (MediaRecorder.isTypeSupported(type)) {
        //                    mimeType = type;
        //                    console.log('Using MIME type:', mimeType);
        //                    break;
        //                }
        //            }

        //            if (!mimeType) {
        //                console.warn('None of the preferred MIME types are supported, using default');
        //            }

        //            const recorderOptions = mimeType ? { mimeType } : {};
        //            mediaRecorder = new MediaRecorder(audioStream, recorderOptions);

        //            mediaRecorder.addEventListener('dataavailable', event => {
        //                if (event.data.size > 0) {
        //                    console.log(`Received audio chunk: size=${event.data.size} type=${event.data.type}`);
        //                    audioChunks.push(event.data);
        //                }
        //            });

        //            mediaRecorder.addEventListener('start', () => {
        //                statusEl.textContent = 'Recording in progress...';
        //                stopBtn.disabled = false;
        //            });

        //            mediaRecorder.addEventListener('stop', sendAudioToServer);

        //            // Start recording - use a time slice to get data during recording
        //            // instead of only at the end, which helps with larger recordings
        //            console.log('Starting MediaRecorder with options:', recorderOptions);
        //            mediaRecorder.start(1000); // Get data every second
        //        } catch (error) {
        //            console.error('Error starting recording:', error);
        //            statusEl.textContent = `Error: ${error.message}`;
        //            startBtn.disabled = false;
        //        }
        //    }

        //    // Stop audio recording
        //    function stopRecording() {
        //        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        //            statusEl.textContent = 'Stopping recording...';
        //            mediaRecorder.stop();
        //            stopBtn.disabled = true;

        //            // Stop the audio tracks
        //            if (audioStream) {
        //                audioStream.getTracks().forEach(track => track.stop());
        //            }

        //            // Stop the visualizer
        //            if (animationId) {
        //                cancelAnimationFrame(animationId);
        //            }
        //        }
        //    }

        //    //convert type to wav
        //    async function convertBlobToWav(blob) {
        //        const arrayBuffer = await blob.arrayBuffer();
        //        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        //        const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);

        //        const numOfChan = audioBuffer.numberOfChannels,
        //            length = audioBuffer.length * numOfChan * 2 + 44,
        //            buffer = new ArrayBuffer(length),
        //            view = new DataView(buffer),
        //            channels = [],
        //            sampleRate = audioBuffer.sampleRate;

        //        let offset = 0;
        //        function setUint16(data) { view.setUint16(offset, data, true); offset += 2; }
        //        function setUint32(data) { view.setUint32(offset, data, true); offset += 4; }

        //        // Write WAV header
        //        setUint32(0x46464952); // "RIFF"
        //        setUint32(length - 8); // file length - 8
        //        setUint32(0x45564157); // "WAVE"

        //        setUint32(0x20746d66); // "fmt " chunk
        //        setUint32(16); // length = 16
        //        setUint16(1); // PCM (uncompressed)
        //        setUint16(numOfChan);
        //        setUint32(sampleRate);
        //        setUint32(sampleRate * 2 * numOfChan); // byte rate
        //        setUint16(numOfChan * 2); // block align
        //        setUint16(16); // bits per sample

        //        setUint32(0x61746164); // "data" chunk
        //        setUint32(length - offset - 4); // data length

        //        // Write interleaved PCM samples
        //        for (let i = 0; i < numOfChan; i++) {
        //            channels.push(audioBuffer.getChannelData(i));
        //        }

        //        for (let i = 0; i < audioBuffer.length; i++) {
        //            for (let ch = 0; ch < numOfChan; ch++) {
        //                let sample = Math.max(-1, Math.min(1, channels[ch][i]));
        //                sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
        //                view.setInt16(offset, sample, true);
        //                offset += 2;
        //            }
        //        }

        //        return new Blob([buffer], { type: 'audio/wav' });
        //    }


        //    // Send recorded audio to the server for transcription
        //    async function sendAudioToServer() {
        //        try {
        //            statusEl.textContent = 'Processing audio...';

        //            const blobMimeType = mediaRecorder ? mediaRecorder.mimeType || 'audio/webm' : 'audio/webm';
        //            console.log('Creating blob with MIME type:', blobMimeType);

        //            console.log(`Number of audio chunks: ${audioChunks.length}`);
        //            audioChunks.forEach((chunk, i) => {
        //                console.log(`Chunk ${i}: size=${chunk.size}, type=${chunk.type}`);
        //            });

        //            if (audioChunks.length === 0) {
        //                throw new Error("No audio data was captured during recording");
        //            }

        //            const rawBlob = new Blob(audioChunks, { type: blobMimeType });
        //            const audioBlob = await convertBlobToWav(rawBlob);
        //            const filename = `recording_${Date.now()}.wav`;


        //            console.log(`Final blob: size=${audioBlob.size}, type=${audioBlob.type}`);

        //            const audioUrl = URL.createObjectURL(audioBlob);
        //            audioPreview.src = audioUrl;
        //            audioPlayback.classList.remove('d-none');

        //            // Create FormData for the server request (only once)
        //            var formData = new FormData();

        //            let fileExtension = '.webm';
        //            if (blobMimeType.includes('webm')) {
        //                fileExtension = '.webm';
        //            } else if (blobMimeType.includes('ogg')) {
        //                fileExtension = '.ogg';
        //            } else if (blobMimeType.includes('mp4')) {
        //                fileExtension = '.mp4';
        //            } else if (blobMimeType.includes('mp3')) {
        //                fileExtension = '.mp3';
        //            }

        //            //const filename = `recording_${Date.now()}${fileExtension}`;
        //            console.log('Using filename:', filename);

        //            formData.append('audioFile', audioBlob, filename);

        //            const tokenElement = document.querySelector('input[name="__RequestVerificationToken"]');
        //            if (tokenElement) {
        //                console.log('Adding antiforgery token to request');
        //                formData.append('__RequestVerificationToken', tokenElement.value);
        //            } else {
        //                console.warn('Antiforgery token not found in the page');
        //            }

        //            console.log('FormData entries:');
        //            for (let pair of formData.entries()) {
        //                console.log(pair[0], pair[1], 'filename:', pair[1].name, 'type:', pair[1].type, 'size:', pair[1].size);
        //            }

        //            statusEl.textContent = 'Sending to server for transcription...';
        //            console.log('Sending request to /Voice/STTs');

        //            try {
        //                const response = await fetch('/Voice/STTs', {
        //                    method: 'POST',
        //                    body: formData // Use the FormData created earlier
        //                });

        //                console.log('Response status:', response.status);

        //                if (!response.ok) {
        //                    const errorText = await response.text();
        //                    console.error('Error response text:', errorText);
        //                    throw new Error(`Server returned ${response.status}`);
        //                }

        //                const data = await response.json();
        //                console.log('Response data:', data);

        //                resultPlaceholder.classList.add('d-none');
        //                resultEl.classList.remove('d-none');
        //                resultEl.textContent = data.transcription || 'No transcription returned';

        //                statusEl.textContent = 'Recording complete. Ready to record again.';
        //                startBtn.disabled = false;
        //            } catch (error) {
        //                console.error('Error sending audio:', error);
        //                statusEl.textContent = `Error: ${error.message}`;
        //                startBtn.disabled = false;

        //                resultPlaceholder.classList.add('d-none');
        //                resultEl.classList.remove('d-none');
        //                resultEl.innerHTML = `<div class="text-danger">Error: ${error.message}</div>`;
        //            }

        //        } catch (error) {
        //            console.error('Error processing audio:', error);
        //            statusEl.textContent = `Error: ${error.message}`;
        //            startBtn.disabled = false;

        //            resultPlaceholder.classList.add('d-none');
        //            resultEl.classList.remove('d-none');
        //            resultEl.innerHTML = `<div class="text-danger">Error: ${error.message}</div>`;
        //        }
        //    }

        //    // Initialize when DOM is loaded
        //    if (document.readyState === 'loading') {
        //        document.addEventListener('DOMContentLoaded', init);
        //    } else {
        //        init();
        //    }
        //})();




        let mediaRecorder;
        let audioChunks = [];
        let audioStream;
        let audioContext;
        let analyser;
        let visualizerCanvas;
        let canvasContext;
        let animationId;
        let silenceTimeout;
        let isRecording = false;

        const micButton = document.getElementById('micButton');
        const statusEl = document.getElementById('status');
        const resultEl = document.getElementById('result');
        const resultPlaceholder = document.getElementById('result-placeholder');
        const visualizer = document.getElementById('visualizer');
        const stopRec = document.getElementById('stop-micButton');

        function setupVisualizer() {
            visualizerCanvas = document.createElement('canvas');
            visualizerCanvas.width = visualizer.clientWidth;
            visualizerCanvas.height = 60;
            visualizer.appendChild(visualizerCanvas);
            canvasContext = visualizerCanvas.getContext('2d');
        }

        function init() {
            setupVisualizer();
            micButton.addEventListener('click', toggleRecording);
            

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                statusEl.textContent = 'Your browser does not support audio recording.';
                micButton.disabled = true;
            }
        }


        function drawVisualizer() {
            if (!analyser) return;

            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            analyser.getByteTimeDomainData(dataArray);

            canvasContext.clearRect(0, 0, visualizerCanvas.width, visualizerCanvas.height);
            canvasContext.lineWidth = 2;
            canvasContext.strokeStyle = '#007bff';
            canvasContext.beginPath();

            const sliceWidth = visualizerCanvas.width / bufferLength;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                const v = dataArray[i] / 128.0;
                const y = v * visualizerCanvas.height / 2;

                if (i === 0) {
                    canvasContext.moveTo(x, y);
                } else {
                    canvasContext.lineTo(x, y);
                }

                x += sliceWidth;
            }

            canvasContext.lineTo(visualizerCanvas.width, visualizerCanvas.height / 2);
            canvasContext.stroke();
            animationId = requestAnimationFrame(drawVisualizer);
        }

        async function toggleRecording() {
            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }

        async function startRecording() {
            try {
                audioChunks = [];
                resultEl.textContent = '';
                resultEl.classList.add('d-none');
                resultPlaceholder.classList.remove('d-none');
                document.getElementById('answer-container').style.display = 'none';

                statusEl.textContent = 'Requesting microphone access...';
                micButton.disabled = true;

                audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(audioStream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 512; //2048
                source.connect(analyser);

                drawVisualizer();

                const mimeType = MediaRecorder.isTypeSupported('audio/webm')
                    ? 'audio/webm'
                    : 'audio/mp4';

                mediaRecorder = new MediaRecorder(audioStream, { mimeType });

                mediaRecorder.addEventListener('dataavailable', event => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                });

                mediaRecorder.addEventListener('start', () => {
                    //isRecording = true;
                    statusEl.textContent = 'Recording in progress... Speak now';
                    micButton.classList.add('recording');
                });

                isRecording = true;

                mediaRecorder.addEventListener('stop', sendAudioToServer);

                mediaRecorder.start(1000);
                startSilenceDetection();
            } catch (error) {
                console.error('Error starting recording:', error);
                statusEl.textContent = `Error: ${error.message}`;
                micButton.disabled = false;
            }
        }

        function startSilenceDetection() {
            const bufferLength = analyser.fftSize;
            const dataArray = new Uint8Array(bufferLength);
            let silenceStart = null;
            const silenceThreshold = 50; 

            function checkSilence() {
                if (!isRecording) return;

                analyser.getByteTimeDomainData(dataArray);
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    let val = (dataArray[i] - 128) / 128;
                    sum += val * val;
                }
                const rms = Math.sqrt(sum / bufferLength) * 100;

                // Debug output
                console.log(`RMS: ${rms.toFixed(2)}, silenceStart: ${silenceStart}`);

                if (rms < silenceThreshold) {
                    if (!silenceStart) {
                        silenceStart = Date.now();
                    } else if (Date.now() - silenceStart > 2000) {
                        console.log("Silence detected, stopping recording.");
                        stopRecording();
                        return;
                    }
                } else {
                    silenceStart = null;
                }

                requestAnimationFrame(checkSilence);
            }

            checkSilence();
        }

        //console.log(`RMS: ${ rms.toFixed(2) }`);

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                statusEl.textContent = 'Processing recording...';
                mediaRecorder.stop();
                micButton.classList.remove('recording');
                isRecording = false;

                if (audioStream) {
                    audioStream.getTracks().forEach(track => track.stop());
                }

                if (animationId) {
                    cancelAnimationFrame(animationId);
                }
            }
        }

        async function convertBlobToWav(blob) {
            const arrayBuffer = await blob.arrayBuffer();
            const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);

            const numOfChan = audioBuffer.numberOfChannels;
            const length = audioBuffer.length * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            const sampleRate = audioBuffer.sampleRate;
            let offset = 0;

            function setUint16(data) {
                view.setUint16(offset, data, true);
                offset += 2;
            }

            function setUint32(data) {
                view.setUint32(offset, data, true);
                offset += 4;
            }

            setUint32(0x46464952);
            setUint32(length - 8);
            setUint32(0x45564157);
            setUint32(0x20746d66);
            setUint32(16);
            setUint16(1);
            setUint16(numOfChan);
            setUint32(sampleRate);
            setUint32(sampleRate * 2 * numOfChan);
            setUint16(numOfChan * 2);
            setUint16(16);
            setUint32(0x61746164);
            setUint32(length - offset - 4);

            for (let i = 0; i < numOfChan; i++) {
                channels.push(audioBuffer.getChannelData(i));
            }

            for (let i = 0; i < audioBuffer.length; i++) {
                for (let ch = 0; ch < numOfChan; ch++) {
                    let sample = Math.max(-1, Math.min(1, channels[ch][i]));
                    sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                    view.setInt16(offset, sample, true);
                    offset += 2;
                }
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        async function sendAudioToServer() {
            try {
                statusEl.textContent = 'Processing audio...';

                if (audioChunks.length === 0) {
                    throw new Error("No audio data was captured during recording");
                }

                const rawBlob = new Blob(audioChunks, { type: mediaRecorder.mimeType });
                const audioBlob = await convertBlobToWav(rawBlob);
                const filename = `recording_${Date.now()}.wav`;

                var formData = new FormData();
                formData.append('audioFile', audioBlob, filename);

                statusEl.textContent = 'Sending to server for transcription...';

                const response = await fetch('/Voice/STTs', {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`Server returned ${response.status}`);
                }

                const data = await response.json();

                resultPlaceholder.classList.add('d-none');
                resultEl.classList.remove('d-none');
                resultEl.textContent = data.transcription || 'No transcription returned';

                const answerContainer = document.getElementById('answer-container');
                const answerText = document.getElementById('answer-text');
                const answerAudio = document.getElementById('answer-audio');

                answerText.textContent = data.answer;
                answerAudio.src = data.audioUrl;
                answerContainer.style.display = 'block';

                answerAudio.play();

                statusEl.textContent = 'Ready to record again';
                micButton.disabled = false;

            } catch (error) {
                console.error('Error processing audio:', error);
                statusEl.textContent = `Error: ${error.message}`;
                micButton.disabled = false;

                resultPlaceholder.classList.add('d-none');
                resultEl.classList.remove('d-none');
                resultEl.innerHTML = `<div class="text-danger">Error: ${error.message}</div>`;
            }
        }

        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', init);
        } else {
            init();
        }


    </script>




